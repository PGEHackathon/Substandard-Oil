{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wLdyeA573zNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credit to:\n",
        "https://github.com/PGEHackathon/workshop/blob/main/05_machine_learning/workflows/PythonDataBasics_MachineLearning.ipynb, for the random tree\n",
        "\n",
        "https://www.sciencedirect.com/science/article/pii/S0920410521006343#fig2, for the uncertainty goodness"
      ],
      "metadata": {
        "id": "d4wjId67y6a3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os                                                 # to set current working directory\n",
        "import math                                               # basic calculations like square root\n",
        "from sklearn.model_selection import train_test_split      # train and test split\n",
        "from sklearn import tree                                  # tree program from scikit learn (package for machine learning)\n",
        "from sklearn.metrics import mean_squared_error            # specific measures to check our models\n",
        "import pandas as pd                                       # DataFrames for tabular data\n",
        "import numpy as np                                        # arrays and matrix math\n",
        "import matplotlib.pyplot as plt                           # general plotting\n",
        "import seaborn as sns                                     # density plots\n",
        "from sklearn import ensemble\n",
        "from scipy.stats import norm\n",
        "\n",
        "#!!!!!!!!!!!!!!!!\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "\n",
        "# Define custom scores to tune Random Forest\n",
        "\n",
        "# Scorer1 scores on MSE\n",
        "def custom_scorer1(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true).ravel()  # Convert y_true to 1D array\n",
        "    y_pred = np.asarray(y_pred).ravel()  # Convert y_pred to 1D array\n",
        "    score = ((y_true - y_pred)**2).mean()\n",
        "    return score\n",
        "\n",
        "# Scorer2 scores on Maldonado-Cruz's uncertainty goodness Xi function\n",
        "def custom_scorer2(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true).ravel()\n",
        "    y_pred = np.asarray(y_pred).ravel()\n",
        "    apd_mean = y_true.mean()\n",
        "    apd_std = y_true.std()\n",
        "    xi = np.zeros(16)\n",
        "    norm_total = 0\n",
        "\n",
        "    pperc = np.linspace(0.1, 0.8, 16)\n",
        "\n",
        "    for i in range(16):\n",
        "        alpha = 1 - pperc[i]  # Convert percentile to alpha\n",
        "        alpha /= 2  # Divide alpha by 2 for two-tailed confidence interval\n",
        "\n",
        "        upper_CI = apd_mean + apd_std * norm.ppf(1 - alpha)\n",
        "        lower_CI = apd_mean - apd_std * norm.ppf(1 - alpha)\n",
        "\n",
        "        for j in range(0, len(y_pred)):\n",
        "          if y_pred[j] < upper_CI and y_pred[j] > lower_CI:\n",
        "            xi[i]=xi[i]+1\n",
        "        xi[i] = xi[i]/(len(y_true))\n",
        "\n",
        "        a = 1 if xi[i] > pperc[i] else 0\n",
        "        norm_total += (((3 * a - 2)) + ((xi[i] - pperc[i]))) * 1 / (len(pperc) + 1)\n",
        "    return abs(norm_total)\n",
        "\n",
        "\n",
        "\n",
        "# # Mean imputation, to be replaced\n",
        "# # Read data\n",
        "# df = pd.read_csv('/content/sample_data/HackathonData2024.csv')\n",
        "\n",
        "# # Find all columns with numeric values\n",
        "# numeric_columns = df.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "# # Replace NaN values with the average of the column\n",
        "# for column in numeric_columns:\n",
        "#   df[column] = df[column].fillna(df[column].mean())\n",
        "\n",
        "# # Save the updated dataframe to a new CSV file\n",
        "# df.to_csv('/content/sample_data/HackathonData2024_updated.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "my_data = pd.read_csv(\"/content/HackData_imputed(2).csv\")\n",
        "my_data = my_data.iloc[:,0:12]\n",
        "numeric_columns = my_data.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "# Define features\n",
        "f1 = 'PARENT_IN_ZONE_MIN_MAP_DIST'\n",
        "f2 = 'PARENT_1050_MEDIAN_WELL_AGE'\n",
        "f3 = 'PARENT_3000_AVG_MAP_DIST'\n",
        "f4 = 'PARENT_3000_WELL_COUNT'\n",
        "f5 = 'PARENT_1050_WELL_COUNT'\n",
        "f6 = 'Soak Time'\n",
        "f7 = 'PARENT_3000_MEDIAN_WELL_AGE'\n",
        "X = my_data[[f1, f2, f3, f4, f5, f6, f7]]\n",
        "y = my_data[['Avg Pump Difference']]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None) # train and test split\n",
        "n_train = len(X_train)\n",
        "n_test = len(X_test)\n",
        "print('Number of training ' + str(n_train) + ', number of test ' + str(n_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate RF\n",
        "random_forest_reg = ensemble.RandomForestRegressor()\n",
        "\n",
        "# Train on training data\n",
        "random_forest_reg.fit(X_train.values, y_train.values)\n",
        "\n",
        "# Predict using training data set\n",
        "y_train_pred = random_forest_reg.predict(X_train.values)\n",
        "\n",
        "# Report the goodness of fit with MSE\n",
        "MSE_train = mean_squared_error(y_train, y_train_pred)\n",
        "\n",
        "\n",
        "\n",
        "print('Variance explained training: %.2f' % MSE_train)\n",
        "\n",
        "y_train_resid = y_train_pred - y_train.values     # calculate the residuals over the training data\n",
        "\n",
        "print('Training: Average error = %.2f' % np.average(y_train_resid)) # calculate the average testing error\n",
        "print('Training: Standard Deviation error = %.2f' % np.std(y_train_resid)) # calculate the standard deviation testing error\n",
        "\n",
        "\n",
        "\n",
        "# Predict using testing data\n",
        "y_test_pred = random_forest_reg.predict(X_test.values)\n",
        "\n",
        "# Report the goodness of fit with MSE\n",
        "MSE_test = mean_squared_error(y_test, y_test_pred)\n",
        "print('Variance explained testing: %.2f' % MSE_test)\n",
        "\n",
        "y_test_resid = y_test_pred - y_test.values     # calculate the residuals over the training data\n",
        "\n",
        "print('Testing: Average error = %.2f' % np.average(y_test_resid)) # calculate the average testing error\n",
        "print('Testing: Standard Deviation error = %.2f' % np.std(y_test_resid)) # calculate the standard deviation testing error\n",
        "\n",
        "\n",
        "\n",
        "# Tuning section\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# Create another RandomForestRegressor\n",
        "rf_regressor = RandomForestRegressor()\n",
        "\n",
        "# Define hyperparameter grid for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'n_estimators': randint(10, 100),\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
        "    'min_samples_split': randint(2, 20),\n",
        "    'min_samples_leaf': randint(1, 20),\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Create RandomizedSearchCV using custom scorers\n",
        "custom_score1 = make_scorer(custom_scorer1, greater_is_better=False)\n",
        "custom_score2 = make_scorer(custom_scorer2, greater_is_better=False)\n",
        "scorers = {'custom_score1': custom_score1, 'custom_score2': custom_score2}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    rf_regressor, param_distributions=param_dist, n_iter=10, cv=5, scoring=custom_score2, random_state=42\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV to training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get and print the best parameters\n",
        "best_params = random_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Access the best model\n",
        "best_rf_model = random_search.best_estimator_\n",
        "\n",
        "# Now, you can use the best_rf_model for predictions on test data\n",
        "predictions = best_rf_model.predict(X_test)\n",
        "\n",
        "\n",
        "# Instantiate the Model with the best parameters\n",
        "random_forest_reg_tuned = random_search.best_estimator_\n",
        "\n",
        "# Fit the Data on Training Data\n",
        "random_forest_reg_tuned.fit(X_train.values, y_train.values)\n",
        "\n",
        "# Make predictions using the training dataset\n",
        "y_train_pred = random_forest_reg_tuned.predict(X_train.values)\n",
        "\n",
        "# Report the goodness of fit\n",
        "MSE_train_tuned = mean_squared_error(y_train, y_train_pred)     # calculate the training MSE\n",
        "print('Variance explained training: %.2f' % MSE_train)\n",
        "\n",
        "y_train_resid = y_train_pred - y_train.values     # calculate the residuals over the training data\n",
        "\n",
        "print('Tuned Model Training: Average error = %.2f' % np.average(y_train_resid)) # calculate the average testing error\n",
        "print('Tuned Model Training: Standard Deviation error = %.2f' % np.std(y_train_resid)) # calculate the standard deviation testing error\n",
        "\n",
        "# Make predictions using the testing dataset\n",
        "y_test_pred = random_forest_reg_tuned.predict(X_test.values)    # predict with the trained model at the training data samples\n",
        "\n",
        "# Report the goodness of fit\n",
        "MSE_test_tuned = mean_squared_error(y_test, y_test_pred)        # calculate the testing MSE\n",
        "print('Tuned Model Testing: Variance explained = %.2f' % MSE_test)\n",
        "\n",
        "y_test_resid = y_test_pred - y_test.values        # calculate the residuals over the testing data\n",
        "\n",
        "print('Tuned Model Testing: Average error = %.2f' % np.average(y_test_resid)) # calculate the average testing error\n",
        "print('Tuned Model Testing: Standard Deviation error = %.2f' % np.std(y_test_resid)) # calculate the standard deviation testing error\n",
        "\n",
        "\n",
        "\n",
        "# Step 1. Instantiate the Model\n",
        "random_forest_reg_final = random_search.best_estimator_ # make the model object and set the hyperparameters\n",
        "\n",
        "# Step 2: Train the Tuned Model with All Data\n",
        "random_forest_reg_final.fit(X.values, y.values) # train (fit) the model with the training data\n",
        "\n",
        "\n",
        "\n",
        "y_test = np.asarray(y_test).ravel()\n",
        "y_test_pred = np.asarray(y_test_pred).ravel()\n",
        "apd_mean = y_test.mean()\n",
        "apd_std = y_test.std()\n",
        "xi = np.zeros(16)\n",
        "norm_total = 0\n",
        "\n",
        "pperc = np.linspace(0.1, 0.8, 16)\n",
        "\n",
        "for i in range(16):\n",
        "    alpha = 1 - pperc[i]  # Convert percentile to alpha\n",
        "    alpha /= 2  # Divide alpha by 2 for two-tailed confidence interval\n",
        "\n",
        "    upper_CI = apd_mean + apd_std * norm.ppf(1-alpha)\n",
        "    lower_CI = apd_mean - apd_std * norm.ppf(1-alpha)\n",
        "\n",
        "    for j in range(0, len(y_test_pred)):\n",
        "      if y_test_pred[j] < upper_CI and y_test_pred[j] > lower_CI:\n",
        "        xi[i]=xi[i]+1\n",
        "    xi[i] = xi[i]/(len(y_test))\n",
        "\n",
        "    a = 1 if xi[i] > pperc[i] else 0\n",
        "    norm_total += (((3 * a - 2)) + ((xi[i] - pperc[i]))) * 1 / (len(pperc) + 1)\n",
        "\n",
        "\n",
        "plt.axline((0, 0), (1, 1))\n",
        "plt.xlabel(\"Percent of distribution within confidence interval\")\n",
        "plt.ylabel(\"Proportion of training predictions within CI\")\n",
        "plt.title(\"Proportion of Data Points within CI vs. Confidence Interval Width for Testing Data\")\n",
        "plt.xlim(0,1); plt.ylim(0,1)\n",
        "plt.scatter(pperc, xi,c='red',alpha=0.2,edgecolor='black')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "y_train = np.asarray(y_train).ravel()\n",
        "y_train_pred = np.asarray(y_train_pred).ravel()\n",
        "apd_mean = y_train.mean()\n",
        "apd_std = y_train.std()\n",
        "xi = np.zeros(16)\n",
        "norm_total = 0\n",
        "\n",
        "pperc = np.linspace(0.1, 0.8, 16)\n",
        "\n",
        "for i in range(16):\n",
        "    alpha = 1 - pperc[i]  # Convert percentile to alpha\n",
        "    alpha /= 2  # Divide alpha by 2 for two-tailed confidence interval\n",
        "\n",
        "    upper_CI = apd_mean + apd_std * norm.ppf(1-alpha)\n",
        "    lower_CI = apd_mean - apd_std * norm.ppf(1-alpha)\n",
        "\n",
        "    for j in range(0, len(y_train_pred)):\n",
        "      if y_train_pred[j] < upper_CI and y_train_pred[j] > lower_CI:\n",
        "        xi[i]=xi[i]+1\n",
        "    xi[i] = xi[i]/(len(y_train))\n",
        "\n",
        "    a = 1 if xi[i] > pperc[i] else 0\n",
        "    norm_total += (((3 * a - 2)) + ((xi[i] - pperc[i]))) * 1 / (len(pperc) + 1)\n",
        "\n",
        "\n",
        "plt.axline((0, 0), (1, 1))\n",
        "plt.xlabel(\"Percent of distribution within confidence interval\")\n",
        "plt.ylabel(\"Proportion of training predictions within CI\")\n",
        "plt.title(\"Proportion of Data Points within CI vs. Confidence Interval Width for Training Data\")\n",
        "\n",
        "plt.xlim(0,1); plt.ylim(0,1)\n",
        "plt.scatter(pperc, xi,c='red',alpha=0.2,edgecolor='black')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# solution = pd.read_csv(\"/content/solution.csv\")\n",
        "# my_data2 = pd.read_csv(\"/content/HackData_imputed (1).csv\")\n",
        "# well_ids = [4, 31, 42, 52, 71, 76, 96, 131, 137, 194, 220, 236, 265, 321, 345]\n",
        "\n",
        "# for j in range(len(well_ids)):\n",
        "#     random_forest_reg_final = random_search.best_estimator_ # make the model object and set the hyperparameters\n",
        "#     random_forest_reg_final.fit(X.values, y.values) # train (fit) the model with the training data\n",
        "#     for i in range(101):\n",
        "#         X_test2 = my_data2.loc[well_ids[j] - 1, [f1, f2, f3, f4, f5, f6, f7]].values.reshape(1, -1)\n",
        "#         y_pred = random_forest_reg_final.predict(X_test2)\n",
        "#         solution.iloc[j, i+1] = y_pred[0]\n",
        "#         random_forest_reg_final = RandomForestRegressor(n_estimators = i+1)\n",
        "#         random_forest_reg_final.fit(X.values, y.values)\n",
        "\n",
        "# solution.to_csv('solution.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "Vt5FyHaxqcpW",
        "outputId": "ee7674b5-a8b3-4008-8f0c-5213d4e1c141"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training 266, number of test 67\n",
            "Variance explained training: 81.26\n",
            "Training: Average error = 0.53\n",
            "Training: Standard Deviation error = 29.90\n",
            "Variance explained testing: 610.64\n",
            "Testing: Average error = 2.63\n",
            "Testing: Standard Deviation error = 27.56\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d81eebd9ce88>\u001b[0m in \u001b[0;36m<cell line: 164>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;31m# Fit the RandomizedSearchCV to training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;31m# Get and print the best parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1769\u001b[0m             ParameterSampler(\n\u001b[1;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \"\"\"\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1248\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pHbrJvBcFtjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: fill cell with given row and column indices\n",
        "\n",
        "df.loc[row_num, col_num] = value\n"
      ],
      "metadata": {
        "id": "pHin7qWwCQWt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}